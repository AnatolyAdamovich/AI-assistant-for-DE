# Data Platform Automation

![UI](infra/ui.png)


Прототип комплексной системы, автоматизирующей полный цикл задач по построению аналитической платформы:
1. Реализация ELT-процессов
2. Моделирование аналитического хранилища
3. Построение дашбордов

**Основная идея** - автоматизация решения типовых задач с помощью Large Language Models через API-интерфейс.


## Architecture

![architecture](infra/arch.png)

Система реализована по модульному принципу и включает 4 основных LLM-компонента:

| Модуль    | Описание |
| -------- | ------- |
| `AnalyticsSpecGenerator` | Модуль для работы с пользовательским вводом. На основе описания желаемой аналитической системы (источники данных, метрики и т.д.) с помощью LLM формируется структурированное техническое задание (AnalyticsSpecification). Включает автоматические рекомендации по метрикам и преобразованиям. |
| `AirflowDagGenerator`  | Модуль для генерации DAG-файлов Airflow. Использует спецификацию из первого модуля для создания пайплайнов (шаблонные вызовы dbt-команд, функции для извлечения и загрузки данных). Итоговый DAG сохраняется в директорию, смонтированную в Docker-контейнер Airflow.    |
| `DbtGenerator`  | Модуль для автоматической генерации dbt-проекта: SQL-модели, документация (yml), тесты качества данных. Использует описание метрик и преобразований из AnalyticsSpecification. Проект сохраняется для последующего запуска в Airflow.    |
| `DashboardGenerator`  |  Модуль для генерации json-описаний дашбордов и визуализаций. Использует REST API Metabase для автоматического создания аналитических отчетов.   |

## Technology Stack

* *Airflow* — организация и управление ELT-процессами
* *dbt* — моделирование и трансформация данных
* *Clickhouse* — аналитическое хранилище данных
* *Metabase* — BI-система для визуализации и построения дашбордов
* *Docker Compose* — контейнеризация и оркестрация сервисов
* *Caila* — прокси-сервис для взаимодействия с LLM
* *Langchain* — доступ к LLM и обработка естественного языка
* *Streamlit* — пользовательский интерфейс

## Setup

1. Установите необходимые библиотеки из файла requirements.txt:

```bash
pip install -r requirements.txt
```

2. В корне проекта создайте файл .env и заполните его следующими переменными окружения:

```
# LLM
OPENAI_API_KEY=your_openai_api_key
BASE_URL=your_base_url

# Data Warehouse (DWH)
DWH_TYPE=your_dwh_type
DWH_HOST=your_dwh_host
DWH_USER=your_dwh_user
DWH_PASS=your_dwh_password
DWH_PORT=your_dwh_port
DWH_DBNAME=your_dwh_dbname
DWH_SCHEMA=your_dwh_schema
DWH_THREADS=your_dwh_threads

# BI (Metabase)
METABASE_URL=your_metabase_url
METABASE_USERNAME=your_metabase_username
METABASE_PASSWORD=your_metabase_password
```

3. Из корневой директории запустите проект с помощью команды:

```bash
streamlit run ui/app.py
```

## Repository Structure

| Директория   | Описание |
| -------- | ------- |
| **src** | Исходный код llm-модулей, модель структурированного ТЗ, конфигурационные файлы для управления проектом, промпты, шаблоны и тесты
| **experiments** | Актуальные jupyter-ноутбуки с экспериментами над прототипом, а также результаты генерации dbt-проектов и airflow-пайплайнов
| **artifacts** | Специальная директория, предназначенная для сохранения генерируемых с помощью llm-модулей файлов. Примонтирована к Docker-контейнеру.
| **infra** | Дополнительные файлы, описывающее используемое окружение
| **ui** | Код пользовательского интерфейса
