{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "240e2117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7658cb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.core.llm_generators.airflow import AirflowDagGenerator\n",
    "from src.core.llm_generators.specification import AnalyticsSpecGenerator\n",
    "from src.config.prompts import prompts\n",
    "from src.config.settings import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "318ca151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnalyticsSpec(business_process=BusinessProcess(name='Анализ продаж интернет-магазина', description='Анализировать продажи и поведение покупателей для повышения выручки и оптимизации маркетинговых кампаний', schedule='0 3 * * *', roles=[{'role': 'Менеджеры по продажам'}, {'role': 'Маркетологи'}, {'role': 'Продуктовый аналитик'}], goals=['Повышение выручки', 'Оптимизация маркетинговых кампаний'], limitations='Ограничения по GDPR'), data_sources=[DataSource(name='orders', description='Таблица заказов', type='database', data_schema={'order_id': 'int', 'product_id': 'int', 'timestamp': 'timestamp', 'customer_id': 'int', 'amount': 'float'}, database='PostgreSQL', access_method='SQL-запросы', data_volume='20000 заказов в день', limitations=None, recommendations=[], connection_params={}), DataSource(name='customers', description='Таблица клиентов', type='database', data_schema={'customer_id': 'int', 'name': 'varchar', 'region_id': 'int', 'age': 'int'}, database='PostgreSQL', access_method='SQL-запросы', data_volume='250000 клиентов', limitations=None, recommendations=[], connection_params={})], metrics=[Metric(name='Общая сумма продаж по дням', description='Сумма продаж за каждый день', calculation_method='SUM(amount) по дням', visualization_method='График', target_value=None, alerting_rules=None), Metric(name='Сумма продаж по регионам', description='Сумма продаж по регионам', calculation_method='SUM(amount) GROUP BY region_id', visualization_method='Диаграмма', target_value=None, alerting_rules=None), Metric(name='Количество уникальных покупателей по регионам', description='Количество уникальных покупателей по регионам', calculation_method='COUNT(DISTINCT customer_id) GROUP BY region_id', visualization_method='Таблица', target_value=None, alerting_rules=None), Metric(name='Средний чек', description='Средний чек по продажам', calculation_method='AVG(amount)', visualization_method='График', target_value=None, alerting_rules=None)], dwh=DWH(database='ClickHouse', environment='dev', structure='Medallion', limitations='Ограничения по GDPR', connection_params={}, retention_policy={}), transformations=[])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec_gen = AnalyticsSpecGenerator()\n",
    "filepath = settings.ARTIFACTS_DIRECTORY / \"analytics_spec.yml\"\n",
    "result = spec_gen._from_yml_to_analytics_spec(filepath)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09c86a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DWH(database='ClickHouse', environment='dev', structure='Medallion', limitations='Ограничения по GDPR', connection_params={}, retention_policy={})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.dwh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4112822",
   "metadata": {},
   "outputs": [],
   "source": [
    "airflow_gen = AirflowDagGenerator(analytics_specification=result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8a787fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'schedule': '0 3 * * *',\n",
       " 'start_date': 'datetime(2023, 10, 1)',\n",
       " 'dag_name': 'analysis_sales_pipeline',\n",
       " 'catchup': False}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = airflow_gen._generate_dag_args()\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e513277d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'code': 'def move_orders_to_dwh(**context):\\n    \"\"\"\\n    Перемещает данные из таблицы заказов (PostgreSQL) в аналитическое хранилище (ClickHouse).\\n    Удаляет предыдущую версию данных в ClickHouse перед загрузкой.\\n\\n    Аргументы:\\n        context: словарь с контекстом выполнения DAG, используется для получения даты выполнения.\\n    \"\"\"\\n    from airflow.hooks.postgres_hook import PostgresHook\\n    from airflow_clickhouse_plugin.hooks.clickhouse_hook import ClickHouseHook\\n    import pandas as pd\\n\\n    # Инициализация подключений\\n    postgres_hook = PostgresHook(postgres_conn_id=\\'source_postgres\\')\\n    clickhouse_hook = ClickHouseHook(clickhouse_conn_id=\\'dwh_clickhouse\\')\\n\\n    # Получение даты выполнения DAG из контекста\\n    execution_date = context[\"ds\"]\\n\\n    # SQL-запрос для извлечения данных из PostgreSQL с фильтрацией по дате\\n    query = \"\"\"\\n        SELECT order_id, product_id, timestamp, customer_id, amount\\n        FROM orders\\n        WHERE DATE(timestamp) = %s\\n    \"\"\"\\n\\n    # Извлечение данных из PostgreSQL\\n    connection = postgres_hook.get_conn()\\n    cursor = connection.cursor()\\n    cursor.execute(query, (execution_date,))\\n    rows = cursor.fetchall()\\n\\n    # Преобразование данных в DataFrame для дальнейшей обработки\\n    columns = [\"order_id\", \"product_id\", \"timestamp\", \"customer_id\", \"amount\"]\\n    df = pd.DataFrame(rows, columns=columns)\\n\\n    # Удаление предыдущей версии данных в ClickHouse\\n    clickhouse_hook.run(\"DROP TABLE IF EXISTS orders_stage\")\\n    clickhouse_hook.run(\\n        \"\"\"\\n        CREATE TABLE orders_stage (\\n            order_id Int32,\\n            product_id Int32,\\n            timestamp DateTime,\\n            customer_id Int32,\\n            amount Float64\\n        )\\n        ENGINE = MergeTree()\\n        ORDER BY timestamp\\n        \"\"\"\\n    )\\n\\n    # Загрузка данных в ClickHouse\\n    clickhouse_hook.insert_df(df, table=\"orders_stage\")\\n'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = airflow_gen._generate_moving_data_function()\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8ff17b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "airflow_gen.generate_dag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ba12095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from datetime import datetime, timedelta\\n\\nfrom airflow.sdk import DAG\\nfrom airflow.operators.bash import BashOperator\\nfrom airflow.operators.python import PythonOperator\\n\\n\\nPROJECT_DIR = \"/opt/airflow/dbt\"\\nDATA_PATH = f\"{PROJECT_DIR}/sample\"\\n\\n\\nDEFAULT_ARGS = {\\n    \"owner\": \"airflow\",\\n    \"depends_on_past\": False,\\n    \"email_on_failure\": False,\\n    \"retries\": 1,\\n    \"retry_delay\": timedelta(minutes=5)\\n}\\n\\n\\n{{ moving_data_from_source_to_dwh }}\\n\\n\\nwith DAG(\\n    dag_id=\"{{ dag_name }}\", \\n    start_date={{ start_date }},\\n    schedule_interval=\"{{ schedule }}\",\\n    max_active_runs=1,\\n    catchup=True\\n) as dag:\\n    \\n    moving_data_from_source_to_dwh = PythonOperator(\\n        task_id=\"moving_data\",\\n        python_callable=moving_data_from_source_to_dwh\\n    )\\n\\n    build_staging_models = BashOperator(\\n        task_id=\"build_staging_models\",\\n        bash_command=f\"dbt run --profiles-dir {PROJECT_DIR} \" \\\\\\n                             f\"--project-dir {PROJECT_DIR} \" \\\\\\n                             f\"--select tag:stage \" \\\\\\n                             f\"--no-version-check \" \\\\\\n    )\\n    \\n    build_intermediate_models = BashOperator(\\n        task_id=\"build_intermediate_models\",\\n        bash_command=f\"dbt run --profiles-dir {PROJECT_DIR} \" \\\\\\n                             f\"--project-dir {PROJECT_DIR} \" \\\\\\n                             f\"--select tag:core \" \\\\\\n                             f\"--no-version-check \" \\\\\\n\\n    )\\n\\n    build_marts_models = BashOperator(\\n        task_id=\"build_marts_models\",\\n        bash_command=f\"dbt run --profiles-dir {PROJECT_DIR} \" \\\\\\n                             f\"--project-dir {PROJECT_DIR} \" \\\\\\n                             f\"--select tag:marts \" \\\\\\n                             f\"--no-version-check \" \\\\\\n    )\\n  \\n    # последовательность задач\\n    moving_data_from_source_to_dwh = moving_data_from_source_to_dwh()\\n    \\n    (\\n        moving_data_from_source_to_dwh\\n        >> build_staging_models\\n        >> build_intermediate_models\\n        >> build_marts_models\\n    )'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template_path = settings.TEMPLATE_DAG_PATH\n",
    "with open(template_path, \"r\", encoding='utf-8') as f:\n",
    "        pipeline_template = f.read()\n",
    "pipeline_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe1a7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = \"\"\"\n",
    "def moving_data_from_source_to_dwh(**context) -> None:\n",
    "\n",
    "    import pandas as pd\n",
    "    from airflow.hooks.postgres_hook import PostgresHook\n",
    "    from airflow_clickhouse_plugin.hooks.clickhouse import ClickHouseHook\n",
    "\n",
    "    # Подключение к источнику данных PostgreSQL\n",
    "    source = PostgresHook(postgres_conn_id='postgres_source')\n",
    "\n",
    "    # Подключение к аналитическому хранилищу ClickHouse\n",
    "    clickhouse_dwh = ClickHouseHook(clickhouse_conn_id='clickhouse_dwh')\n",
    "\n",
    "    # Извлечение данных из таблицы 'orders'\n",
    "    orders_query = \"SELECT * FROM orders\"\n",
    "    orders_records = source.get_records(orders_query)\n",
    "\n",
    "    # Извлечение данных из таблицы 'customers'\n",
    "    customers_query = \"SELECT * FROM customers\"\n",
    "    customers_records = source.get_records(customers_query)\n",
    "\n",
    "    # Загрузка данных в ClickHouse\n",
    "    clickhouse_dwh.execute(\"CREATE TABLE IF NOT EXISTS orders (order_id Int32, product_id Int32, timestamp DateTime, customer_id Int32, amount Float64) ENGINE = MergeTree() ORDER BY order_id\")\n",
    "    clickhouse_dwh.execute(\"CREATE TABLE IF NOT EXISTS customers (customer_id Int32, name String, region_id Int32, age Int32) ENGINE = MergeTree() ORDER BY customer_id\")\n",
    "\n",
    "    clickhouse_dwh.execute('INSERT INTO orders VALUES', orders_records)\n",
    "    clickhouse_dwh.execute('INSERT INTO customers VALUES', customers_records)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64c708f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from datetime import datetime, timedelta\\n\\nfrom airflow.sdk import DAG\\nfrom airflow.operators.bash import BashOperator\\nfrom airflow.operators.python import PythonOperator\\n\\n\\nPROJECT_DIR = \"/opt/airflow/dbt\"\\nDATA_PATH = f\"{PROJECT_DIR}/sample\"\\n\\n\\nDEFAULT_ARGS = {\\n    \"owner\": \"airflow\",\\n    \"depends_on_past\": False,\\n    \"email_on_failure\": False,\\n    \"retries\": 1,\\n    \"retry_delay\": timedelta(minutes=5)\\n}\\n\\n\\ndef moving_data_from_source_to_dwh(**context) -> None:\\n\\n    import pandas as pd\\n    from airflow.hooks.postgres_hook import PostgresHook\\n    from airflow_clickhouse_plugin.hooks.clickhouse import ClickHouseHook\\n\\n    # Подключение к источнику данных PostgreSQL\\n    source = PostgresHook(postgres_conn_id=\\'postgres_source\\')\\n\\n    # Подключение к аналитическому хранилищу ClickHouse\\n    clickhouse_dwh = ClickHouseHook(clickhouse_conn_id=\\'clickhouse_dwh\\')\\n\\n    # Извлечение данных из таблицы \\'orders\\'\\n    orders_query = \"SELECT * FROM orders\"\\n    orders_records = source.get_records(orders_query)\\n\\n    # Извлечение данных из таблицы \\'customers\\'\\n    customers_query = \"SELECT * FROM customers\"\\n    customers_records = source.get_records(customers_query)\\n\\n    # Загрузка данных в ClickHouse\\n    clickhouse_dwh.execute(\"CREATE TABLE IF NOT EXISTS orders (order_id Int32, product_id Int32, timestamp DateTime, customer_id Int32, amount Float64) ENGINE = MergeTree() ORDER BY order_id\")\\n    clickhouse_dwh.execute(\"CREATE TABLE IF NOT EXISTS customers (customer_id Int32, name String, region_id Int32, age Int32) ENGINE = MergeTree() ORDER BY customer_id\")\\n\\n    clickhouse_dwh.execute(\\'INSERT INTO orders VALUES\\', orders_records)\\n    clickhouse_dwh.execute(\\'INSERT INTO customers VALUES\\', customers_records)\\n\\n\\nwith DAG(\\n    dag_id=\"example_dag\", \\n    start_date=datetime(2025, 12, 14),\\n    schedule_interval=\"0 5 * * *\",\\n    max_active_runs=1,\\n    catchup=True\\n) as dag:\\n    \\n    moving_data_from_source_to_dwh = PythonOperator(\\n        task_id=\"moving_data\",\\n        python_callable=moving_data_from_source_to_dwh\\n    )\\n\\n    build_staging_models = BashOperator(\\n        task_id=\"build_staging_models\",\\n        bash_command=f\"dbt run --profiles-dir {PROJECT_DIR} \" \\\\\\n                             f\"--project-dir {PROJECT_DIR} \" \\\\\\n                             f\"--select tag:stage \" \\\\\\n                             f\"--no-version-check \" \\\\\\n    )\\n    \\n    build_intermediate_models = BashOperator(\\n        task_id=\"build_intermediate_models\",\\n        bash_command=f\"dbt run --profiles-dir {PROJECT_DIR} \" \\\\\\n                             f\"--project-dir {PROJECT_DIR} \" \\\\\\n                             f\"--select tag:core \" \\\\\\n                             f\"--no-version-check \" \\\\\\n\\n    )\\n\\n    build_marts_models = BashOperator(\\n        task_id=\"build_marts_models\",\\n        bash_command=f\"dbt run --profiles-dir {PROJECT_DIR} \" \\\\\\n                             f\"--project-dir {PROJECT_DIR} \" \\\\\\n                             f\"--select tag:marts \" \\\\\\n                             f\"--no-version-check \" \\\\\\n    )\\n  \\n    # последовательность задач\\n    moving_data_from_source_to_dwh = moving_data_from_source_to_dwh()\\n    \\n    (\\n        moving_data_from_source_to_dwh\\n        >> build_staging_models\\n        >> build_intermediate_models\\n        >> build_marts_models\\n    )'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "airflow_gen._render_dag(pipeline_template=pipeline_template,\n",
    "                        dag_name=\"example_dag\",\n",
    "                        start_date=\"datetime(2025, 12, 14)\",\n",
    "                        schedule=\"0 5 * * *\",\n",
    "                        moving_data_from_source_to_dwh=code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5086981",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\"\"\n",
    "def moving_data_from_source_to_dwh(**context) -> None:\n",
    "    '''\n",
    "    Описание\n",
    "    '''\n",
    "    from airflow.hooks.postgres_hook import PostgresHook\n",
    "    from airflow_clickhouse_plugin.hooks.clickhouse import ClickHouseHook\n",
    "\n",
    "    # источник данных\n",
    "    source = PostgresHook(postgres_conn_id='postgres_source')\n",
    "\n",
    "    # аналитическое хранилище\n",
    "    clickhouse_dwh = ClickHouseHook(clickhouse_conn_id='clickhouse_dwh')\n",
    "    ds = context[\"ds\"]\n",
    "    # выгрузка данных\n",
    "    query1 = \"SELECT column1, column2, timestamp FROM schema.table1 WHERE timestamp::date =\" + ds\n",
    "    records1 = source.get_records(query1)\n",
    "    query2 = \"SELECT column1, column2 FROM schema.table2\"\n",
    "    records2 = source.get_records(query2)\n",
    "\n",
    "    # загрузка данных\n",
    "    clickhouse_dwh.execute(\"DROP TABLE table1 IF EXISTS\")\n",
    "    clickhouse_dwh.execute(\"CREATE TABLE tabl1 (column1 ..., column2 ..., )\")\n",
    "    clickhouse_dwh.execute('INSERT INTO table1 VALUES', records1)\n",
    "    clickhouse_dwh.execute(\"DROP TABLE table2 IF EXISTS\")\n",
    "    clickhouse_dwh.execute(\"CREATE TABLE tabl2 (column1 ..., column2 ..., )\")\n",
    "    clickhouse_dwh.execute('INSERT INTO table2 VALUES', records2)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b76dcf9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef moving_data_from_source_to_dwh(**context) -> None:\\n    \\'\\'\\'\\n    Описание\\n    \\'\\'\\'\\n    from airflow.hooks.postgres_hook import PostgresHook\\n    from airflow_clickhouse_plugin.hooks.clickhouse import ClickHouseHook\\n\\n    # источник данных\\n    source = PostgresHook(postgres_conn_id=\\'postgres_source\\')\\n\\n    # аналитическое хранилище\\n    clickhouse_dwh = ClickHouseHook(clickhouse_conn_id=\\'clickhouse_dwh\\')\\n    ds = context[\"ds\"]\\n    # выгрузка данных\\n    query = \"SELECT column1, column2, timestamp FROM schema.table_name WHERE timestamp::date =\" + ds\\n    records = source.get_records(records)\\n\\n    # загрузка данных\\n    clickhouse_dwh.execute(\"DROP TABLE table IF EXISTS\")\\n    clickhouse_dwh.execute(\"CREATE TABLE table (column1 ..., column2 ..., )\")\\n    clickhouse_dwh.execute(\\'INSERT INTO table VALUES\\', records)'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edc4f90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
