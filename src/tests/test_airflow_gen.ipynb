{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "240e2117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7658cb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.core.llm_generators.airflow import AirflowDagGenerator\n",
    "from src.core.llm_generators.specification import AnalyticsSpecGenerator\n",
    "from src.config.prompts import prompts\n",
    "from src.config.settings import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "318ca151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnalyticsSpec(business_process=BusinessProcess(name='Анализ продаж интернет-магазина', description='Анализировать продажи и поведение покупателей для повышения выручки и оптимизации маркетинговых кампаний', schedule='0 3 * * *', roles=[{'role': 'Менеджеры по продажам'}, {'role': 'Маркетологи'}, {'role': 'Продуктовый аналитик'}], goals=['Повышение выручки', 'Оптимизация маркетинговых кампаний'], limitations='Ограничения по GDPR'), data_sources=[DataSource(name='orders', description='Таблица заказов', type='database', data_schema={'order_id': 'int', 'product_id': 'int', 'timestamp': 'timestamp', 'customer_id': 'int', 'amount': 'float'}, database='PostgreSQL', access_method='SQL-запросы', data_volume='20000 заказов в день', limitations=None, recommendations=[], connection_params={}), DataSource(name='customers', description='Таблица клиентов', type='database', data_schema={'customer_id': 'int', 'name': 'varchar', 'region_id': 'int', 'age': 'int'}, database='PostgreSQL', access_method='SQL-запросы', data_volume='250000 клиентов', limitations=None, recommendations=[], connection_params={})], metrics=[Metric(name='Общая сумма продаж по дням', description='Сумма продаж за каждый день', calculation_method='SUM(amount) по дням', visualization_method='График', target_value=None, alerting_rules=None), Metric(name='Сумма продаж по регионам', description='Сумма продаж по регионам', calculation_method='SUM(amount) GROUP BY region_id', visualization_method='Диаграмма', target_value=None, alerting_rules=None), Metric(name='Количество уникальных покупателей по регионам', description='Количество уникальных покупателей по регионам', calculation_method='COUNT(DISTINCT customer_id) GROUP BY region_id', visualization_method='Таблица', target_value=None, alerting_rules=None), Metric(name='Средний чек', description='Средний чек по продажам', calculation_method='AVG(amount)', visualization_method='График', target_value=None, alerting_rules=None)], dwh=DWH(database='ClickHouse', environment='dev', structure='Medallion', limitations='Ограничения по GDPR', connection_params={}, retention_policy={}), transformations=[])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec_gen = AnalyticsSpecGenerator()\n",
    "filepath = settings.ARTIFACTS_DIRECTORY / \"analytics_spec.yml\"\n",
    "result = spec_gen._from_yml_to_analytics_spec(filepath)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09c86a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DWH(database='ClickHouse', environment='dev', structure='Medallion', limitations='Ограничения по GDPR', connection_params={}, retention_policy={})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.dwh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4112822",
   "metadata": {},
   "outputs": [],
   "source": [
    "airflow_gen = AirflowDagGenerator(analytics_specification=result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8a787fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'schedule': '0 3 * * *',\n",
       " 'start_date': 'datetime(2023, 10, 1)',\n",
       " 'dag_name': 'analysis_sales_pipeline',\n",
       " 'catchup': False}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = airflow_gen._generate_dag_args()\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e513277d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'code': 'def move_orders_to_dwh(**context):\\n    \"\"\"\\n    Перемещает данные из таблицы заказов (PostgreSQL) в аналитическое хранилище (ClickHouse).\\n    Удаляет предыдущую версию данных в ClickHouse перед загрузкой.\\n\\n    Аргументы:\\n        context: словарь с контекстом выполнения DAG, используется для получения даты выполнения.\\n    \"\"\"\\n    from airflow.hooks.postgres_hook import PostgresHook\\n    from airflow_clickhouse_plugin.hooks.clickhouse_hook import ClickHouseHook\\n    import pandas as pd\\n\\n    # Инициализация подключений\\n    postgres_hook = PostgresHook(postgres_conn_id=\\'source_postgres\\')\\n    clickhouse_hook = ClickHouseHook(clickhouse_conn_id=\\'dwh_clickhouse\\')\\n\\n    # Получение даты выполнения DAG из контекста\\n    execution_date = context[\"ds\"]\\n\\n    # SQL-запрос для извлечения данных из PostgreSQL с фильтрацией по дате\\n    query = \"\"\"\\n        SELECT order_id, product_id, timestamp, customer_id, amount\\n        FROM orders\\n        WHERE DATE(timestamp) = %s\\n    \"\"\"\\n\\n    # Извлечение данных из PostgreSQL\\n    connection = postgres_hook.get_conn()\\n    cursor = connection.cursor()\\n    cursor.execute(query, (execution_date,))\\n    rows = cursor.fetchall()\\n\\n    # Преобразование данных в DataFrame для дальнейшей обработки\\n    columns = [\"order_id\", \"product_id\", \"timestamp\", \"customer_id\", \"amount\"]\\n    df = pd.DataFrame(rows, columns=columns)\\n\\n    # Удаление предыдущей версии данных в ClickHouse\\n    clickhouse_hook.run(\"DROP TABLE IF EXISTS orders_stage\")\\n    clickhouse_hook.run(\\n        \"\"\"\\n        CREATE TABLE orders_stage (\\n            order_id Int32,\\n            product_id Int32,\\n            timestamp DateTime,\\n            customer_id Int32,\\n            amount Float64\\n        )\\n        ENGINE = MergeTree()\\n        ORDER BY timestamp\\n        \"\"\"\\n    )\\n\\n    # Загрузка данных в ClickHouse\\n    clickhouse_hook.insert_df(df, table=\"orders_stage\")\\n'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = airflow_gen._generate_moving_data_function()\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8ff17b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "airflow_gen.generate_dag()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4da1a6b",
   "metadata": {},
   "source": [
    "# Проверки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95d4ff8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adal==1.2.7\\nadlfs==2024.2.0\\nagate==1.9.1\\naiobotocore==2.11.2\\naiofiles==23.2.1\\naiohttp==3.9.3\\naioitertools==0.11.0\\naiosignal==1.3.1\\nairflow-clickhouse-plugin==1.4.0\\nalembic==1.13.1\\namqp==5.2.0\\nannotated-types==0.7.0\\nanyio==4.3.0\\napache-airflow==2.8.3\\napache-airflow-providers-amazon==8.19.0\\napache-airflow-providers-celery==3.6.1\\napache-airflow-providers-cncf-kubernetes==8.0.1\\napache-airflow-providers-common-io==1.3.0\\napache-airflow-providers-common-sql==1.11.1\\napache-airflow-providers-docker==3.9.2\\napache-airflow-providers-elasticsearch==5.3.3\\napache-airflow-providers-ftp==3.7.0\\napache-airflow-providers-google==10.16.0\\napache-airflow-providers-grpc==3.4.1\\napache-airflow-providers-hashicorp==3.6.4\\napache-airflow-providers-http==4.10.0\\napache-airflow-providers-imap==3.5.0\\napache-airflow-providers-microsoft-azure==9.0.1\\napache-airflow-providers-mysql==5.5.4\\napache-airflow-providers-odbc==4.4.1\\napache-airflow-providers-openlineage==1.6.0\\napache-airflow-providers-postgres==5.10.2\\napache-airflow-providers-redis==3.6.0\\napache-airflow-providers-sendgrid==3.4.0\\napache-airflow-providers-sftp==4.9.0\\napache-airflow-providers-slack==8.6.1\\napache-airflow-providers-smtp==1.6.1\\napache-airflow-providers-snowflake==5.3.1\\napache-airflow-providers-sqlite==3.7.1\\napache-airflow-providers-ssh==3.10.1\\napispec==6.5.0\\nargcomplete==3.2.3\\nasgiref==3.7.2\\nasn1crypto==1.5.1\\nasync-timeout==4.0.3\\nasyncssh==2.14.2\\nattrs==23.2.0\\nAuthlib==1.3.0\\nazure-batch==14.1.0\\nazure-common==1.1.28\\nazure-core==1.30.1\\nazure-cosmos==4.5.1\\nazure-datalake-store==0.0.53\\nazure-identity==1.15.0\\nazure-keyvault-secrets==4.8.0\\nazure-kusto-data==4.3.1\\nazure-mgmt-containerinstance==10.1.0\\nazure-mgmt-containerregistry==10.3.0\\nazure-mgmt-core==1.4.0\\nazure-mgmt-cosmosdb==9.4.0\\nazure-mgmt-datafactory==6.0.0\\nazure-mgmt-datalake-nspkg==3.0.1\\nazure-mgmt-datalake-store==0.5.0\\nazure-mgmt-nspkg==3.0.2\\nazure-mgmt-resource==23.0.1\\nazure-mgmt-storage==21.1.0\\nazure-nspkg==3.0.2\\nazure-servicebus==7.12.0\\nazure-storage-blob==12.19.1\\nazure-storage-file-datalake==12.14.0\\nazure-storage-file-share==12.15.0\\nazure-synapse-artifacts==0.18.0\\nazure-synapse-spark==0.7.0\\nBabel==2.14.0\\nbackoff==2.2.1\\nbcrypt==4.1.2\\nbeautifulsoup4==4.12.3\\nbilliard==4.2.0\\nblinker==1.7.0\\nboto3==1.33.13\\nbotocore==1.33.13\\ncachelib==0.9.0\\ncachetools==5.3.3\\ncattrs==23.2.3\\ncelery==5.3.6\\ncertifi==2024.2.2\\ncffi==1.16.0\\nchardet==5.2.0\\ncharset-normalizer==3.3.2\\nclick==8.1.7\\nclick-didyoumean==0.3.0\\nclick-plugins==1.1.1\\nclick-repl==0.3.0\\nclickclick==20.10.2\\nclickhouse-connect==0.7.19\\nclickhouse-driver==0.2.9\\ncolorama==0.4.6\\ncolorlog==4.8.0\\nConfigUpdater==3.2\\nconnexion==2.14.2\\ncron-descriptor==1.4.3\\ncroniter==2.0.2\\ncryptography==41.0.7\\ndaff==1.4.2\\ndb-dtypes==1.2.0\\ndbt-adapters==1.9.0\\ndbt-clickhouse==1.8.0\\ndbt-common==1.12.0\\ndbt-core==1.8.5\\ndbt-extractor==0.6.0\\ndbt-semantic-interfaces==0.5.1\\ndecorator==5.1.1\\ndeepdiff==7.0.1\\nDeprecated==1.2.14\\ndill==0.3.1.1\\ndistlib==0.3.8\\ndnspython==2.6.1\\ndocker==7.0.0\\ndocutils==0.20.1\\nelastic-transport==8.12.0\\nelasticsearch==8.12.1\\nemail-validator==1.3.1\\neventlet==0.35.2\\nexceptiongroup==1.2.0\\nfilelock==3.13.1\\nFlask==2.2.5\\nFlask-AppBuilder==4.3.11\\nFlask-Babel==2.0.0\\nFlask-Caching==2.1.0\\nFlask-JWT-Extended==4.6.0\\nFlask-Limiter==3.5.1\\nFlask-Login==0.6.3\\nFlask-Session==0.5.0\\nFlask-SQLAlchemy==2.5.1\\nFlask-WTF==1.2.1\\nflower==2.0.1\\nfrozenlist==1.4.1\\nfsspec==2024.2.0\\ngcloud-aio-auth==4.2.3\\ngcloud-aio-bigquery==7.1.0\\ngcloud-aio-storage==9.2.0\\ngcsfs==2024.2.0\\ngevent==24.2.1\\ngoogle-ads==23.1.0\\ngoogle-analytics-admin==0.22.7\\ngoogle-api-core==2.17.1\\ngoogle-api-python-client==2.122.0\\ngoogle-auth==2.28.2\\ngoogle-auth-httplib2==0.2.0\\ngoogle-auth-oauthlib==1.2.0\\ngoogle-cloud-aiplatform==1.43.0\\ngoogle-cloud-appengine-logging==1.4.3\\ngoogle-cloud-audit-log==0.2.5\\ngoogle-cloud-automl==2.13.3\\ngoogle-cloud-batch==0.17.14\\ngoogle-cloud-bigquery==3.19.0\\ngoogle-cloud-bigquery-datatransfer==3.15.1\\ngoogle-cloud-bigtable==2.23.0\\ngoogle-cloud-build==3.23.3\\ngoogle-cloud-compute==1.18.0\\ngoogle-cloud-container==2.43.0\\ngoogle-cloud-core==2.4.1\\ngoogle-cloud-datacatalog==3.18.3\\ngoogle-cloud-dataflow-client==0.8.10\\ngoogle-cloud-dataform==0.5.9\\ngoogle-cloud-dataplex==1.12.3\\ngoogle-cloud-dataproc==5.9.3\\ngoogle-cloud-dataproc-metastore==1.15.3\\ngoogle-cloud-dlp==3.16.0\\ngoogle-cloud-kms==2.21.3\\ngoogle-cloud-language==2.13.3\\ngoogle-cloud-logging==3.9.0\\ngoogle-cloud-memcache==1.9.3\\ngoogle-cloud-monitoring==2.19.3\\ngoogle-cloud-orchestration-airflow==1.12.1\\ngoogle-cloud-os-login==2.14.3\\ngoogle-cloud-pubsub==2.20.1\\ngoogle-cloud-redis==2.15.3\\ngoogle-cloud-resource-manager==1.12.3\\ngoogle-cloud-run==0.10.5\\ngoogle-cloud-secret-manager==2.18.3\\ngoogle-cloud-spanner==3.43.0\\ngoogle-cloud-speech==2.25.1\\ngoogle-cloud-storage==2.15.0\\ngoogle-cloud-storage-transfer==1.11.3\\ngoogle-cloud-tasks==2.16.3\\ngoogle-cloud-texttospeech==2.16.3\\ngoogle-cloud-translate==3.15.3\\ngoogle-cloud-videointelligence==2.13.3\\ngoogle-cloud-vision==3.7.2\\ngoogle-cloud-workflows==1.14.3\\ngoogle-crc32c==1.5.0\\ngoogle-re2==1.1\\ngoogle-resumable-media==2.7.0\\ngoogleapis-common-protos==1.63.0\\ngraphviz==0.20.1\\ngreenlet==3.0.3\\ngrpc-google-iam-v1==0.13.0\\ngrpc-interceptor==0.15.4\\ngrpcio==1.62.1\\ngrpcio-gcp==0.2.2\\ngrpcio-status==1.62.1\\ngunicorn==21.2.0\\nh11==0.14.0\\nhttpcore==0.16.3\\nhttplib2==0.22.0\\nhttpx==0.23.3\\nhumanize==4.9.0\\nhvac==2.1.0\\nidna==3.6\\nijson==3.2.3\\nimportlib-metadata==6.11.0\\nimportlib_resources==6.1.3\\ninflection==0.5.1\\nisodate==0.6.1\\nitsdangerous==2.1.2\\nJinja2==3.1.3\\njmespath==0.10.0\\njson-merge-patch==0.2\\njsonpath-ng==1.6.1\\njsonschema==4.21.1\\njsonschema-specifications==2023.12.1\\nkombu==5.3.5\\nkubernetes==29.0.0\\nkubernetes_asyncio==29.0.0\\nlazy-object-proxy==1.10.0\\nldap3==2.9.1\\nleather==0.4.0\\nlimits==3.10.0\\nlinkify-it-py==2.0.3\\nlockfile==0.12.2\\nLogbook==1.5.3\\nlooker-sdk==24.2.0\\nlxml==5.1.0\\nlz4==4.4.4\\nMako==1.3.2\\nmarkdown-it-py==3.0.0\\nMarkupSafe==2.1.5\\nmarshmallow==3.21.1\\nmarshmallow-oneofschema==3.1.1\\nmarshmallow-sqlalchemy==0.26.1\\nmashumaro==3.15\\nmdit-py-plugins==0.4.0\\nmdurl==0.1.2\\nminimal-snowplow-tracker==0.0.2\\nmore-itertools==10.2.0\\nmsal==1.27.0\\nmsal-extensions==1.1.0\\nmsgpack==1.1.0\\nmsrest==0.7.1\\nmsrestazure==0.6.4\\nmultidict==6.0.5\\nmysql-connector-python==8.3.0\\nmysqlclient==2.2.4\\nnetworkx==3.4.2\\nnumpy==1.24.4\\noauthlib==3.2.2\\nopenlineage-integration-common==1.9.1\\nopenlineage-python==1.9.1\\nopenlineage_sql==1.9.1\\nopentelemetry-api==1.23.0\\nopentelemetry-exporter-otlp==1.23.0\\nopentelemetry-exporter-otlp-proto-common==1.23.0\\nopentelemetry-exporter-otlp-proto-grpc==1.23.0\\nopentelemetry-exporter-otlp-proto-http==1.23.0\\nopentelemetry-proto==1.23.0\\nopentelemetry-sdk==1.23.0\\nopentelemetry-semantic-conventions==0.44b0\\nordered-set==4.1.0\\npackaging==23.2\\npandas==2.1.4\\npandas-gbq==0.22.0\\nparamiko==3.4.0\\nparsedatetime==2.6\\npathspec==0.12.1\\npendulum==3.0.0\\nplatformdirs==3.11.0\\npluggy==1.4.0\\nply==3.11\\nportalocker==2.8.2\\nprison==0.2.1\\nprometheus_client==0.20.0\\nprompt-toolkit==3.0.43\\nproto-plus==1.23.0\\nprotobuf==4.25.3\\npsutil==5.9.8\\npsycopg2-binary==2.9.9\\npyarrow==14.0.2\\npyasn1==0.5.1\\npyasn1-modules==0.3.0\\nPyAthena==3.3.0\\npycparser==2.21\\npydantic==2.11.4\\npydantic_core==2.33.2\\npydata-google-auth==1.8.2\\nPygments==2.17.2\\nPyJWT==2.8.0\\nPyNaCl==1.5.0\\npyodbc==5.1.0\\npyOpenSSL==24.1.0\\npyparsing==3.1.2\\npython-daemon==3.0.1\\npython-dateutil==2.9.0.post0\\npython-dotenv==1.0.1\\npython-http-client==3.3.7\\npython-ldap==3.4.4\\npython-nvd3==0.15.0\\npython-slugify==8.0.4\\npytimeparse==1.1.8\\npytz==2024.1\\nPyYAML==6.0.1\\nredis==4.6.0\\nredshift-connector==2.1.0\\nreferencing==0.33.0\\nrequests==2.31.0\\nrequests-oauthlib==1.4.0\\nrequests-toolbelt==1.0.0\\nrfc3339-validator==0.1.4\\nrfc3986==1.5.0\\nrich==13.7.1\\nrich-argparse==1.4.0\\nrpds-py==0.18.0\\nrsa==4.9\\ns3transfer==0.8.2\\nscramp==1.4.4\\nsendgrid==6.11.0\\nsetproctitle==1.3.3\\nshapely==2.0.3\\nsix==1.16.0\\nslack_sdk==3.27.1\\nsniffio==1.3.1\\nsnowflake-connector-python==3.7.1\\nsnowflake-sqlalchemy==1.5.1\\nsortedcontainers==2.4.0\\nsoupsieve==2.5\\nSQLAlchemy==1.4.52\\nsqlalchemy-bigquery==1.10.0\\nSQLAlchemy-JSONField==1.0.2\\nsqlalchemy-redshift==0.8.14\\nsqlalchemy-spanner==1.6.2\\nSQLAlchemy-Utils==0.41.1\\nsqlparse==0.5.3\\nsshtunnel==0.4.0\\nstarkbank-ecdsa==2.2.0\\nstatsd==4.0.1\\ntabulate==0.9.0\\ntenacity==8.2.3\\ntermcolor==2.4.0\\ntext-unidecode==1.3\\ntime-machine==2.14.0\\ntomlkit==0.12.4\\ntornado==6.4\\ntyping-inspection==0.4.0\\ntyping_extensions==4.13.2\\ntzdata==2024.1\\ntzlocal==5.3.1\\nuc-micro-py==1.0.3\\nunicodecsv==0.14.1\\nuniversal-pathlib==0.1.4\\nuritemplate==4.1.1\\nurllib3==2.0.7\\nvine==5.1.0\\nvirtualenv==20.25.1\\nwatchtower==3.1.0\\nwcwidth==0.2.13\\nwebsocket-client==1.7.0\\nWerkzeug==2.2.3\\nwrapt==1.16.0\\nWTForms==3.1.2\\nyarl==1.9.4\\nzipp==3.17.0\\nzope.event==5.0\\nzope.interface==6.2\\nzstandard==0.23.0\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(settings.PROJECT_ROOT / \"infra\" / \"requirements.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    requirements = f.read()\n",
    "requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb0d8af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ba12095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from datetime import datetime, timedelta\\n\\nfrom airflow.sdk import DAG\\nfrom airflow.operators.bash import BashOperator\\nfrom airflow.operators.python import PythonOperator\\n\\n\\nPROJECT_DIR = \"/opt/airflow/dbt\"\\nDATA_PATH = f\"{PROJECT_DIR}/sample\"\\n\\n\\nDEFAULT_ARGS = {\\n    \"owner\": \"airflow\",\\n    \"depends_on_past\": False,\\n    \"email_on_failure\": False,\\n    \"retries\": 1,\\n    \"retry_delay\": timedelta(minutes=5)\\n}\\n\\n\\n{{ moving_data_from_source_to_dwh }}\\n\\n\\nwith DAG(\\n    dag_id=\"{{ dag_name }}\", \\n    start_date={{ start_date }},\\n    schedule_interval=\"{{ schedule }}\",\\n    max_active_runs=1,\\n    catchup=True\\n) as dag:\\n    \\n    moving_data_from_source_to_dwh = PythonOperator(\\n        task_id=\"moving_data\",\\n        python_callable=moving_data_from_source_to_dwh\\n    )\\n\\n    build_staging_models = BashOperator(\\n        task_id=\"build_staging_models\",\\n        bash_command=f\"dbt run --profiles-dir {PROJECT_DIR} \" \\\\\\n                             f\"--project-dir {PROJECT_DIR} \" \\\\\\n                             f\"--select tag:stage \" \\\\\\n                             f\"--no-version-check \" \\\\\\n    )\\n    \\n    build_intermediate_models = BashOperator(\\n        task_id=\"build_intermediate_models\",\\n        bash_command=f\"dbt run --profiles-dir {PROJECT_DIR} \" \\\\\\n                             f\"--project-dir {PROJECT_DIR} \" \\\\\\n                             f\"--select tag:core \" \\\\\\n                             f\"--no-version-check \" \\\\\\n\\n    )\\n\\n    build_marts_models = BashOperator(\\n        task_id=\"build_marts_models\",\\n        bash_command=f\"dbt run --profiles-dir {PROJECT_DIR} \" \\\\\\n                             f\"--project-dir {PROJECT_DIR} \" \\\\\\n                             f\"--select tag:marts \" \\\\\\n                             f\"--no-version-check \" \\\\\\n    )\\n  \\n    # последовательность задач\\n    moving_data_from_source_to_dwh = moving_data_from_source_to_dwh()\\n    \\n    (\\n        moving_data_from_source_to_dwh\\n        >> build_staging_models\\n        >> build_intermediate_models\\n        >> build_marts_models\\n    )'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template_path = settings.TEMPLATE_DAG_PATH\n",
    "with open(template_path, \"r\", encoding='utf-8') as f:\n",
    "        pipeline_template = f.read()\n",
    "pipeline_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe1a7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = \"\"\"\n",
    "def moving_data_from_source_to_dwh(**context) -> None:\n",
    "\n",
    "    import pandas as pd\n",
    "    from airflow.hooks.postgres_hook import PostgresHook\n",
    "    from airflow_clickhouse_plugin.hooks.clickhouse import ClickHouseHook\n",
    "\n",
    "    # Подключение к источнику данных PostgreSQL\n",
    "    source = PostgresHook(postgres_conn_id='postgres_source')\n",
    "\n",
    "    # Подключение к аналитическому хранилищу ClickHouse\n",
    "    clickhouse_dwh = ClickHouseHook(clickhouse_conn_id='clickhouse_dwh')\n",
    "\n",
    "    # Извлечение данных из таблицы 'orders'\n",
    "    orders_query = \"SELECT * FROM orders\"\n",
    "    orders_records = source.get_records(orders_query)\n",
    "\n",
    "    # Извлечение данных из таблицы 'customers'\n",
    "    customers_query = \"SELECT * FROM customers\"\n",
    "    customers_records = source.get_records(customers_query)\n",
    "\n",
    "    # Загрузка данных в ClickHouse\n",
    "    clickhouse_dwh.execute(\"CREATE TABLE IF NOT EXISTS orders (order_id Int32, product_id Int32, timestamp DateTime, customer_id Int32, amount Float64) ENGINE = MergeTree() ORDER BY order_id\")\n",
    "    clickhouse_dwh.execute(\"CREATE TABLE IF NOT EXISTS customers (customer_id Int32, name String, region_id Int32, age Int32) ENGINE = MergeTree() ORDER BY customer_id\")\n",
    "\n",
    "    clickhouse_dwh.execute('INSERT INTO orders VALUES', orders_records)\n",
    "    clickhouse_dwh.execute('INSERT INTO customers VALUES', customers_records)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64c708f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from datetime import datetime, timedelta\\n\\nfrom airflow.sdk import DAG\\nfrom airflow.operators.bash import BashOperator\\nfrom airflow.operators.python import PythonOperator\\n\\n\\nPROJECT_DIR = \"/opt/airflow/dbt\"\\nDATA_PATH = f\"{PROJECT_DIR}/sample\"\\n\\n\\nDEFAULT_ARGS = {\\n    \"owner\": \"airflow\",\\n    \"depends_on_past\": False,\\n    \"email_on_failure\": False,\\n    \"retries\": 1,\\n    \"retry_delay\": timedelta(minutes=5)\\n}\\n\\n\\ndef moving_data_from_source_to_dwh(**context) -> None:\\n\\n    import pandas as pd\\n    from airflow.hooks.postgres_hook import PostgresHook\\n    from airflow_clickhouse_plugin.hooks.clickhouse import ClickHouseHook\\n\\n    # Подключение к источнику данных PostgreSQL\\n    source = PostgresHook(postgres_conn_id=\\'postgres_source\\')\\n\\n    # Подключение к аналитическому хранилищу ClickHouse\\n    clickhouse_dwh = ClickHouseHook(clickhouse_conn_id=\\'clickhouse_dwh\\')\\n\\n    # Извлечение данных из таблицы \\'orders\\'\\n    orders_query = \"SELECT * FROM orders\"\\n    orders_records = source.get_records(orders_query)\\n\\n    # Извлечение данных из таблицы \\'customers\\'\\n    customers_query = \"SELECT * FROM customers\"\\n    customers_records = source.get_records(customers_query)\\n\\n    # Загрузка данных в ClickHouse\\n    clickhouse_dwh.execute(\"CREATE TABLE IF NOT EXISTS orders (order_id Int32, product_id Int32, timestamp DateTime, customer_id Int32, amount Float64) ENGINE = MergeTree() ORDER BY order_id\")\\n    clickhouse_dwh.execute(\"CREATE TABLE IF NOT EXISTS customers (customer_id Int32, name String, region_id Int32, age Int32) ENGINE = MergeTree() ORDER BY customer_id\")\\n\\n    clickhouse_dwh.execute(\\'INSERT INTO orders VALUES\\', orders_records)\\n    clickhouse_dwh.execute(\\'INSERT INTO customers VALUES\\', customers_records)\\n\\n\\nwith DAG(\\n    dag_id=\"example_dag\", \\n    start_date=datetime(2025, 12, 14),\\n    schedule_interval=\"0 5 * * *\",\\n    max_active_runs=1,\\n    catchup=True\\n) as dag:\\n    \\n    moving_data_from_source_to_dwh = PythonOperator(\\n        task_id=\"moving_data\",\\n        python_callable=moving_data_from_source_to_dwh\\n    )\\n\\n    build_staging_models = BashOperator(\\n        task_id=\"build_staging_models\",\\n        bash_command=f\"dbt run --profiles-dir {PROJECT_DIR} \" \\\\\\n                             f\"--project-dir {PROJECT_DIR} \" \\\\\\n                             f\"--select tag:stage \" \\\\\\n                             f\"--no-version-check \" \\\\\\n    )\\n    \\n    build_intermediate_models = BashOperator(\\n        task_id=\"build_intermediate_models\",\\n        bash_command=f\"dbt run --profiles-dir {PROJECT_DIR} \" \\\\\\n                             f\"--project-dir {PROJECT_DIR} \" \\\\\\n                             f\"--select tag:core \" \\\\\\n                             f\"--no-version-check \" \\\\\\n\\n    )\\n\\n    build_marts_models = BashOperator(\\n        task_id=\"build_marts_models\",\\n        bash_command=f\"dbt run --profiles-dir {PROJECT_DIR} \" \\\\\\n                             f\"--project-dir {PROJECT_DIR} \" \\\\\\n                             f\"--select tag:marts \" \\\\\\n                             f\"--no-version-check \" \\\\\\n    )\\n  \\n    # последовательность задач\\n    moving_data_from_source_to_dwh = moving_data_from_source_to_dwh()\\n    \\n    (\\n        moving_data_from_source_to_dwh\\n        >> build_staging_models\\n        >> build_intermediate_models\\n        >> build_marts_models\\n    )'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "airflow_gen._render_dag(pipeline_template=pipeline_template,\n",
    "                        dag_name=\"example_dag\",\n",
    "                        start_date=\"datetime(2025, 12, 14)\",\n",
    "                        schedule=\"0 5 * * *\",\n",
    "                        moving_data_from_source_to_dwh=code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5086981",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\"\"\n",
    "def moving_data_from_source_to_dwh(**context) -> None:\n",
    "    '''\n",
    "    Описание\n",
    "    '''\n",
    "    from airflow.hooks.postgres_hook import PostgresHook\n",
    "    from airflow_clickhouse_plugin.hooks.clickhouse import ClickHouseHook\n",
    "\n",
    "    # источник данных\n",
    "    source = PostgresHook(postgres_conn_id='postgres_source')\n",
    "\n",
    "    # аналитическое хранилище\n",
    "    clickhouse_dwh = ClickHouseHook(clickhouse_conn_id='clickhouse_dwh')\n",
    "    ds = context[\"ds\"]\n",
    "    # выгрузка данных\n",
    "    query1 = \"SELECT column1, column2, timestamp FROM schema.table1 WHERE timestamp::date =\" + ds\n",
    "    records1 = source.get_records(query1)\n",
    "    query2 = \"SELECT column1, column2 FROM schema.table2\"\n",
    "    records2 = source.get_records(query2)\n",
    "\n",
    "    # загрузка данных\n",
    "    clickhouse_dwh.execute(\"DROP TABLE table1 IF EXISTS\")\n",
    "    clickhouse_dwh.execute(\"CREATE TABLE tabl1 (column1 ..., column2 ..., )\")\n",
    "    clickhouse_dwh.execute('INSERT INTO table1 VALUES', records1)\n",
    "    clickhouse_dwh.execute(\"DROP TABLE table2 IF EXISTS\")\n",
    "    clickhouse_dwh.execute(\"CREATE TABLE tabl2 (column1 ..., column2 ..., )\")\n",
    "    clickhouse_dwh.execute('INSERT INTO table2 VALUES', records2)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b76dcf9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef moving_data_from_source_to_dwh(**context) -> None:\\n    \\'\\'\\'\\n    Описание\\n    \\'\\'\\'\\n    from airflow.hooks.postgres_hook import PostgresHook\\n    from airflow_clickhouse_plugin.hooks.clickhouse import ClickHouseHook\\n\\n    # источник данных\\n    source = PostgresHook(postgres_conn_id=\\'postgres_source\\')\\n\\n    # аналитическое хранилище\\n    clickhouse_dwh = ClickHouseHook(clickhouse_conn_id=\\'clickhouse_dwh\\')\\n    ds = context[\"ds\"]\\n    # выгрузка данных\\n    query = \"SELECT column1, column2, timestamp FROM schema.table_name WHERE timestamp::date =\" + ds\\n    records = source.get_records(records)\\n\\n    # загрузка данных\\n    clickhouse_dwh.execute(\"DROP TABLE table IF EXISTS\")\\n    clickhouse_dwh.execute(\"CREATE TABLE table (column1 ..., column2 ..., )\")\\n    clickhouse_dwh.execute(\\'INSERT INTO table VALUES\\', records)'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edc4f90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
