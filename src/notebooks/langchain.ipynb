{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3eb16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = \"<YOUR API_KEY>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08bfd86",
   "metadata": {},
   "source": [
    "# База"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01260abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"just-ai/claude/claude-3-5-sonnet\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=TOKEN,\n",
    "    base_url=\"https://caila.io/api/adapters/openai\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c9fefdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"Translate the following from English into {language}\"\n",
    "user_template = \"Hello everybody\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template),\n",
    "     (\"user\", user_template)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "076bbe4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Translate the following from English into Russian', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello everybody', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.invoke({'language': 'Russian'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb7479df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Здравствуйте все / Привет всем\\n\\nNote: \"Здравствуйте\" is more formal, while \"Привет\" is informal.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 17, 'total_tokens': 59, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'claude-3-5-sonnet-20241022', 'system_fingerprint': None, 'id': '8d3d438d-8c71-4e79-bfe0-5122ce4a11a3', 'finish_reason': None, 'logprobs': None}, id='run-6ee1df59-ede3-4a62-a3df-2c0769059bea-0', usage_metadata={'input_tokens': 17, 'output_tokens': 42, 'total_tokens': 59, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt_template | llm\n",
    "chain.invoke(\n",
    "    {\n",
    "        'language': 'Russian'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed2c54c",
   "metadata": {},
   "source": [
    "# Генерация кода для Export-шага"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90dc1c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"just-ai/openai-proxy/gpt-4.1\"\n",
    "TEMPERATURE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c5f431a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=MODEL,\n",
    "    temperature=TEMPERATURE,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=TOKEN,\n",
    "    base_url=\"https://caila.io/api/adapters/openai\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c9350b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class DataSource(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    data_schema: Dict[str, str] # column_name: type\n",
    "    type: str  # 'table', 'csv', 'api', etc.\n",
    "    database: str\n",
    "    access_method: str | None = None\n",
    "    limitations: Optional[str] | None = None\n",
    "    recommendations: List[str] | None = None\n",
    "    connection_params: Dict[str, str] | None = None\n",
    "\n",
    "\n",
    "ds_customers = DataSource(\n",
    "        name=\"customers\",\n",
    "        description=\"Таблица клиентов\",\n",
    "        data_schema={\"customer_id\": \"Int64\", \n",
    "                     \"name\": \"Text\", \n",
    "                     \"position\": \"Text\",\n",
    "                     \"Age\": \"Int64\",\n",
    "                     \"tariff\": \"Json\"},\n",
    "        type=\"table\",\n",
    "        database=\"ClickHouse\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b702b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = (\n",
    "    \"You are an experienced Python data engineer writing code for an Airflow DAG. \" \\\n",
    ")\n",
    "\n",
    "# user_template = (\n",
    "#     \"Implement a function def export_data_from_source(**context) -> None for an Airflow DAG.\\n\"\n",
    "#     \"- The data source has the following properties:\\n\"\n",
    "#     \"  - database: {database}\\n\"\n",
    "#     \"  - table name: {name}\\n\"\n",
    "#     \"  - data schema: {data_schema}\\n\"\n",
    "#     \"- Use the appropriate Airflow connection for the database type (for example, PostgresHook(\\\"<name>_source\\\") for PostgreSQL or ClickHouseHook(\\\"<name>_source)\\\" for Clickhouse).\\n\"\n",
    "#     \"- Use only standard and popular open-source Python libraries (such as pandas, psycopg2). \\n\"\n",
    "#     \"- Save the extracted data to a file in a suitable format (CSV, JSON, Parquet, etc). Preferably CSV.\\n\"\n",
    "#     \"- Add a docstring in Russian that describes what the function does.\\n\"\n",
    "#     \"- Do not add any comments or explanations outside the function code.\\n\"\n",
    "#     \"- Return only the function code.\"\n",
    "# )\n",
    "\n",
    "user_template = (\n",
    "    \"Implement a function def export_and_load_data_from_source(**context) -> None for an Airflow DAG.\\n\"\n",
    "    \"- The data source has the following properties: {data_source} \\n\"\n",
    "    \"- Use the appropriate Airflow connection for the database type (for example, PostgresHook(\\\"<name>_source\\\") for PostgreSQL or ClickHouseHook(\\\"<name>_source)\\\" for Clickhouse).\\n\"\n",
    "    \"- Use only standard and popular open-source Python libraries (such as pandas, psycopg2). \\n\"\n",
    "    \"- Save the extracted data to a file in a suitable format (CSV, JSON, Parquet, etc). Preferably CSV.\\n\"\n",
    "    \"- Add a docstring in Russian that describes what the function does.\\n\"\n",
    "    \"- Do not add any comments or explanations outside the function code.\\n\"\n",
    "    \"- Return only the function code.\"\n",
    ")\n",
    "\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template),\n",
    "     (\"user\", user_template)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f53dcafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | llm\n",
    "# result = chain.invoke(\n",
    "#     {\"database\": ds_customers.database,\n",
    "#      \"name\": ds_customers.name,\n",
    "#      \"data_schema\": ds_customers.data_schema}\n",
    "# )\n",
    "\n",
    "result = chain.invoke(\n",
    "    {\"data_source\": ds_customers}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8022f8b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```python\\ndef export_data_from_source(**context) -> None:\\n    \"\"\"\\n    Экспортирует данные из источника ClickHouse, используя соответствующий Airflow Hook.\\n    Данные из таблицы \\'customers\\' сохраняются в формате CSV.\\n    \"\"\"\\n    from airflow.providers.clickhouse.hooks.clickhouse import ClickHouseHook\\n    import pandas as pd\\n\\n    # Создание подключения к ClickHouse\\n    hook = ClickHouseHook(\\'clickhouse_default\\')\\n\\n    # SQL запрос для выборки данных\\n    sql = \"SELECT * FROM customers\"\\n\\n    # Выполнение запроса и загрузка данных в DataFrame\\n    df = hook.get_pandas_df(sql)\\n\\n    # Сохранение данных в CSV файл\\n    df.to_csv(\\'/path/to/output/customers_data.csv\\', index=False)\\n```'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c7d8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_code(code_str: str) -> str:\n",
    "    # убрать обрамление ``` или ```python и оставить только содержимое\n",
    "    pattern = r\"```(?:python)?\\n(.*?)```\"\n",
    "    matches = re.findall(pattern, code_str, re.DOTALL)\n",
    "    if matches:\n",
    "        # если несколько блоков, объединяем их через 2 перевода строки\n",
    "        return \"\\n\\n\".join(match.strip() for match in matches)\n",
    "    return code_str.strip()\n",
    "\n",
    "cleaned_result = clean_code(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad18c3dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def export_data_from_source(**context) -> None:\\n    \"\"\"\\n    Экспортирует данные из источника ClickHouse, используя соответствующий Airflow Hook.\\n    Данные из таблицы \\'customers\\' сохраняются в формате CSV.\\n    \"\"\"\\n    from airflow.providers.clickhouse.hooks.clickhouse import ClickHouseHook\\n    import pandas as pd\\n\\n    # Создание подключения к ClickHouse\\n    hook = ClickHouseHook(\\'clickhouse_default\\')\\n\\n    # SQL запрос для выборки данных\\n    sql = \"SELECT * FROM customers\"\\n\\n    # Выполнение запроса и загрузка данных в DataFrame\\n    df = hook.get_pandas_df(sql)\\n\\n    # Сохранение данных в CSV файл\\n    df.to_csv(\\'/path/to/output/customers_data.csv\\', index=False)'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a0d749f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"results/export/airflow_template_3_{MODEL.split('/')[-1].replace('-', '_')}.py\", \n",
    "          \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(cleaned_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684b1f12",
   "metadata": {},
   "source": [
    "# Собираем цепочку для генерации пайплайна"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b9b7dc",
   "metadata": {},
   "source": [
    "#### разбираем airflow args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55017946",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "MODEL = \"just-ai/openai-proxy/gpt-4o\"\n",
    "TEMPERATURE = 0\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=MODEL,\n",
    "    temperature=TEMPERATURE,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=TOKEN,\n",
    "    base_url=\"https://caila.io/api/adapters/openai\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c498583",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class BusinessProcess(BaseModel):\n",
    "    name: str = \"\"\n",
    "    schedule: str = \"\"\n",
    "\n",
    "business_process = BusinessProcess(\n",
    "    name=\"Аналитика заказов интернет-магазина\",\n",
    "    schedule=\"Ежедневно по ночам (в промежуток с 01:00 по 03:00), начиная с 1 февраля 2025 года\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3e3882b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерируем schedule_interval и start_date на основе business_process\n",
    "system_template = (\n",
    "    \"You are an experienced data engineer. Your task is to choose the correct Airflow schedule_interval and start_date for a DAG, \"\n",
    "    \"based on the following business process and recommendations. \\n\"\n",
    "    \"Return only the values in Python code format\"\n",
    "    \"(for example: schedule_interval=\\\"@daily\\\"\\nstart_date=datetime(2024, 1, 1)).\"\n",
    ")\n",
    "user_template = (\n",
    "    \"Business process:\\n\"\n",
    "    \"- name: {name}\\n\"\n",
    "    \"- schedule: {schedule}\\n\"\n",
    ")\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),\n",
    "    (\"user\", user_template)\n",
    "])\n",
    "chain = prompt_template | llm\n",
    "\n",
    "result_for_args = chain.invoke({\n",
    "    \"name\": getattr(business_process, \"name\", \"Анализ\"),\n",
    "    \"schedule\": getattr(business_process, \"schedule\", \"0 0 * * *\"),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e984e401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'schedule_interval=\"0 1 * * *\"\\nstart_date=datetime(2025, 2, 1)'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_args_code = clean_code(result_for_args.content)\n",
    "cleaned_args_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "70e8b157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['schedule_interval=\"0 1 * * *\"', 'start_date=datetime(2025, 2, 1)']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_args_code.splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a2733d",
   "metadata": {},
   "source": [
    "#### добавляем код для функции moving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc489d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class DataSource(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    data_schema: Dict[str, str] # column_name: type\n",
    "    type: str  # 'table', 'csv', 'api', etc.\n",
    "    database: str\n",
    "    access_method: str | None = None\n",
    "    limitations: Optional[str] | None = None\n",
    "    recommendations: List[str] | None = None\n",
    "    connection_params: Dict[str, str] | None = None\n",
    "\n",
    "\n",
    "ds_orders = DataSource(\n",
    "    name=\"orders\",\n",
    "    description=\"Таблица заказов\",\n",
    "    data_schema={\"order_id\": \"Int64\",\n",
    "                 \"product_id\": \"Int64\",\n",
    "                 \"timestamp\": \"datetime\",\n",
    "                 \"customer_id\": \"Int64\",\n",
    "                 \"money\": \"numeric\"},\n",
    "    type=\"table\",\n",
    "    database=\"PostgreSQL\",\n",
    "    access_method=None,\n",
    "    recommendations=[\"использовать фильтрацию по дате (timestamp)\"]\n",
    ")\n",
    "\n",
    "ds = [ds_customers, ds_orders]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b76ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "MODEL = \"just-ai/gemini/gemini-2.5-pro\"\n",
    "# MODEL = \"just-ai/openai-proxy/gpt-4o\"\n",
    "TEMPERATURE = 0\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=MODEL,\n",
    "    temperature=TEMPERATURE,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=TOKEN,\n",
    "    base_url=\"https://caila.io/api/adapters/openai\"\n",
    ")\n",
    "\n",
    "system_template = (\n",
    "    \"You are an experienced Python data engineer writing code for an Airflow DAG. \" \\\n",
    ")\n",
    "\n",
    "user_template = (\n",
    "    \"Implement a function def moving_data_from_source_to_dwh(**context) -> None for an Airflow DAG.\\n\"\n",
    "    \"- The data source has the following properties:\\n\"\n",
    "    \"  - database: {database}\\n\"\n",
    "    \"  - table name: {name}\\n\"\n",
    "    \"  - data schema: {data_schema}\\n\"\n",
    "    \"- For analytics database use ClickHouseHook('clickhouse_dwh')\"\n",
    "    \"- Use the appropriate Airflow connection for the database type (for example, PostgresHook(\\\"<name>_source\\\") for PostgreSQL or ClickHouseHook(\\\"<name>_source)\\\" for Clickhouse).\\n\"\n",
    "    \"- Use only standard and popular open-source Python libraries (such as pandas, psycopg2). \\n\"\n",
    "    \"- Add a docstring in Russian that describes what the function does.\\n\"\n",
    "    \"- Import all needed libraries inside function.\\n\"\n",
    "    \"- Do not add any comments, code or explanations outside the function code.\\n\"\n",
    "    \"- Return only the function code.\"\n",
    ")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template),\n",
    "     (\"user\", user_template)]\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm\n",
    "\n",
    "result_moving = chain.invoke(\n",
    "    {\"database\": ds_orders.database,\n",
    "     \"name\": ds_orders.name,\n",
    "     \"data_schema\": ds_orders.data_schema}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f480a5d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def moving_data_from_source_to_dwh(**context) -> None:\\n    \"\"\"\\n    Извлекает данные из таблицы \\'orders\\' в источнике PostgreSQL\\n    и загружает их в таблицу \\'orders\\' в ClickHouse DWH.\\n\\n    Предполагается, что структура таблицы \\'orders\\' в ClickHouse\\n    соответствует схеме источника:\\n    (order_id Int64, product_id Int64, timestamp DateTime, customer_id Int64, money Decimal).\\n    \"\"\"\\n    import pandas as pd\\n    from airflow.providers.postgres.hooks.postgres import PostgresHook\\n    from airflow.providers.clickhouse.hooks.clickhouse import ClickHouseHook\\n\\n    # Define connection IDs and table names\\n    postgres_conn_id = \"postgres_source\"  # Replace with your actual Postgres connection ID\\n    clickhouse_conn_id = \"clickhouse_dwh\"\\n    source_table = \"orders\"\\n    target_table = \"orders\" # Assuming the target table has the same name\\n\\n    # Define source schema explicitly for clarity, though hook handles types\\n    source_schema = {\\n        \\'order_id\\': \\'Int64\\',\\n        \\'product_id\\': \\'Int64\\',\\n        \\'timestamp\\': \\'datetime64[ns]\\', # Pandas equivalent for datetime\\n        \\'customer_id\\': \\'Int64\\',\\n        \\'money\\': \\'object\\' # Pandas often reads numeric/decimal as object initially\\n    }\\n    target_columns = [\\'order_id\\', \\'product_id\\', \\'timestamp\\', \\'customer_id\\', \\'money\\']\\n\\n    # Create hooks\\n    pg_hook = PostgresHook(postgres_conn_id=postgres_conn_id)\\n    ch_hook = ClickHouseHook(clickhouse_conn_id=clickhouse_conn_id)\\n\\n    # Extract data from PostgreSQL\\n    sql = f\"SELECT {\\', \\'.join(target_columns)} FROM {source_table}\"\\n    source_conn = pg_hook.get_conn()\\n    \\n    try:\\n        # Use pandas read_sql for better type handling potential with numeric/decimal\\n        df = pd.read_sql(sql, source_conn)\\n        \\n        # Optional: Explicit type casting if needed, especially for decimals\\n        # df[\\'money\\'] = pd.to_numeric(df[\\'money\\']) # Cast if read as string/object\\n        # Ensure correct types based on source schema if read_sql inference isn\\'t perfect\\n        # df = df.astype(source_schema, errors=\\'ignore\\') # Apply schema types\\n\\n    finally:\\n        if source_conn:\\n            source_conn.close()\\n\\n    # Load data into ClickHouse\\n    if not df.empty:\\n        # Convert DataFrame to list of lists/tuples for insertion\\n        # Handle potential NaT values for datetime columns if necessary before conversion\\n        df[\\'timestamp\\'] = df[\\'timestamp\\'].astype(object).where(df[\\'timestamp\\'].notnull(), None)\\n        rows_to_insert = df.values.tolist()\\n\\n        ch_hook.insert_rows(\\n            table=target_table,\\n            rows=rows_to_insert,\\n            target_fields=target_columns\\n        )\\n    else:\\n        # Optionally log that no data was found\\n        pass # Or use context[\\'ti\\'].log.info(\"No data extracted from source.\")'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_code_moving = clean_code(result_moving.content)\n",
    "cleaned_code_moving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "dda6f33c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def moving_data_from_source_to_dwh(**context) -> None:\\n    \"\"\"\\n    Извлекает данные из таблицы \\'orders\\' в источнике PostgreSQL\\n    и загружает их в таблицу \\'orders\\' в ClickHouse DWH.\\n\\n    Предполагается, что структура таблицы \\'orders\\' в ClickHouse\\n    соответствует схеме источника:\\n    (order_id Int64, product_id Int64, timestamp DateTime, customer_id Int64, money Decimal).\\n    \"\"\"\\n    import pandas as pd\\n    from airflow.providers.postgres.hooks.postgres import PostgresHook\\n    from airflow.providers.clickhouse.hooks.clickhouse import ClickHouseHook\\n\\n    # Define connection IDs and table names\\n    postgres_conn_id = \"postgres_source\"  # Replace with your actual Postgres connection ID\\n    clickhouse_conn_id = \"clickhouse_dwh\"\\n    source_table = \"orders\"\\n    target_table = \"orders\" # Assuming the target table has the same name\\n\\n    # Define source schema explicitly for clarity, though hook handles types\\n    source_schema = {\\n        \\'order_id\\': \\'Int64\\',\\n        \\'product_id\\': \\'Int64\\',\\n        \\'timestamp\\': \\'datetime64[ns]\\', # Pandas equivalent for datetime\\n        \\'customer_id\\': \\'Int64\\',\\n        \\'money\\': \\'object\\' # Pandas often reads numeric/decimal as object initially\\n    }\\n    target_columns = [\\'order_id\\', \\'product_id\\', \\'timestamp\\', \\'customer_id\\', \\'money\\']\\n\\n    # Create hooks\\n    pg_hook = PostgresHook(postgres_conn_id=postgres_conn_id)\\n    ch_hook = ClickHouseHook(clickhouse_conn_id=clickhouse_conn_id)\\n\\n    # Extract data from PostgreSQL\\n    sql = f\"SELECT {\\', \\'.join(target_columns)} FROM {source_table}\"\\n    source_conn = pg_hook.get_conn()\\n    \\n    try:\\n        # Use pandas read_sql for better type handling potential with numeric/decimal\\n        df = pd.read_sql(sql, source_conn)\\n        \\n        # Optional: Explicit type casting if needed, especially for decimals\\n        # df[\\'money\\'] = pd.to_numeric(df[\\'money\\']) # Cast if read as string/object\\n        # Ensure correct types based on source schema if read_sql inference isn\\'t perfect\\n        # df = df.astype(source_schema, errors=\\'ignore\\') # Apply schema types\\n\\n    finally:\\n        if source_conn:\\n            source_conn.close()\\n\\n    # Load data into ClickHouse\\n    if not df.empty:\\n        # Convert DataFrame to list of lists/tuples for insertion\\n        # Handle potential NaT values for datetime columns if necessary before conversion\\n        df[\\'timestamp\\'] = df[\\'timestamp\\'].astype(object).where(df[\\'timestamp\\'].notnull(), None)\\n        rows_to_insert = df.values.tolist()\\n\\n        ch_hook.insert_rows(\\n            table=target_table,\\n            rows=rows_to_insert,\\n            target_fields=target_columns\\n        )\\n    else:\\n        # Optionally log that no data was found\\n        pass # Or use context[\\'ti\\'].log.info(\"No data extracted from source.\")'"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_code_moving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84815721",
   "metadata": {},
   "source": [
    "##### всё сохраняем в одном месте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "2ad57828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from datetime import datetime, timedelta\\n\\nfrom airflow.decorators import dag, task\\n\\n\\nPROJECT_DIR = \"/opt/airflow/dbt\"\\nDATA_PATH = f\"{PROJECT_DIR}/sample\"\\nSEED_PATH = f\"{PROJECT_DIR}/seeds\"\\n\\nDEFAULT_ARGS = {\\n    \"owner\": \"airflow\",\\n    \"depends_on_past\": False,\\n    \"email_on_failure\": False,\\n    \"retries\": 1,\\n    \"retry_delay\": timedelta(minutes=5)\\n}\\n\\n\\n@dag(\\n    default_args=DEFAULT_ARGS,\\n    max_active_runs=1,\\n    schedule_interval=\"0 * * * *\",\\n    start_date=datetime(2025, 1, 1),\\n    catchup=True\\n)\\ndef airflow_pipeline():\\n\\n    @task\\n    def moving_data_from_source_to_dwh(**context) -> None:\\n        pass\\n\\n    @task.bash\\n    def build_staging_models() -> str:\\n        bash_command=f\"dbt run --profiles-dir {PROJECT_DIR} \" \\\\\\n                             f\"--project-dir {PROJECT_DIR} \" \\\\\\n                             f\"--select tag:stage\" \\\\\\n                             f\"--no-version-check \" \\\\\\n        \\n        return bash_command\\n    \\n    @task.bash\\n    def build_intermediate_models() -> str:\\n        bash_command=f\"dbt run --profiles-dir {PROJECT_DIR} \" \\\\\\n                             f\"--project-dir {PROJECT_DIR} \" \\\\\\n                             f\"--select tag:intermediate\" \\\\\\n                             f\"--no-version-check \" \\\\\\n        \\n        return bash_command\\n\\n    @task.bash\\n    def build_marts_models() -> str:\\n        bash_command=f\"dbt run --profiles-dir {PROJECT_DIR} \" \\\\\\n                             f\"--project-dir {PROJECT_DIR} \" \\\\\\n                             f\"--select tag:marts\" \\\\\\n                             f\"--no-version-check \" \\\\\\n        \\n        return bash_command\\n  \\n    # последовательность задач\\n    moving_data_from_source_to_dwh = moving_data_from_source_to_dwh()\\n    build_staging_models = build_staging_models()\\n    build_intermediate_models = build_intermediate_models()\\n    build_marts_models = build_marts_models()\\n    \\n    (\\n        moving_data_from_source_to_dwh\\n        >> build_staging_models\\n        >> build_intermediate_models\\n        >> build_marts_models\\n    )\\n\\n\\nairflow_pipeline = airflow_pipeline()'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../templates/airflow_dag_template.py', \"r\", encoding=\"utf-8\") as f:\n",
    "    template = f.read()\n",
    "\n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "6244ebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ПОДСТАВЛЯЕМ schedule_interval и start_date\n",
    "for line in cleaned_args_code.splitlines():\n",
    "    if \"schedule_interval\" in line:\n",
    "        template = re.sub(r'schedule_interval\\s*=\\s*[\"\\']{0,1}[\"\\']{0,1}', line, template)\n",
    "    if \"start_date\" in line:\n",
    "        template = re.sub(r'start_date\\s*=\\s*datetime\\([^)]+\\)', line, template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "9beb51ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from datetime import datetime, timedelta\\n\\nfrom airflow.decorators import dag, task\\n\\n\\nPROJECT_DIR = \"/opt/airflow/dbt\"\\nDATA_PATH = f\"{PROJECT_DIR}/sample\"\\nSEED_PATH = f\"{PROJECT_DIR}/seeds\"\\n\\nDEFAULT_ARGS = {\\n    \"owner\": \"airflow\",\\n    \"depends_on_past\": False,\\n    \"email_on_failure\": False,\\n    \"retries\": 1,\\n    \"retry_delay\": timedelta(minutes=5)\\n}\\n\\n\\n@dag(\\n    default_args=DEFAULT_ARGS,\\n    max_active_runs=1,\\n    schedule_interval=\"0 1 * * *\"0 * * * *\",\\n    start_date=datetime(2025, 2, 1),\\n    catchup=True\\n)\\ndef airflow_pipeline():\\n\\n    @task\\n    def moving_data_from_source_to_dwh(**context) -> None:\\n        pass\\n\\n    @task.bash\\n    def build_staging_models() -> str:\\n        bash_command=f\"dbt run --profiles-dir {PROJECT_DIR} \" \\\\\\n                             f\"--project-dir {PROJECT_DIR} \" \\\\\\n                             f\"--select tag:stage\" \\\\\\n                             f\"--no-version-check \" \\\\\\n        \\n        return bash_command\\n    \\n    @task.bash\\n    def build_intermediate_models() -> str:\\n        bash_command=f\"dbt run --profiles-dir {PROJECT_DIR} \" \\\\\\n                             f\"--project-dir {PROJECT_DIR} \" \\\\\\n                             f\"--select tag:intermediate\" \\\\\\n                             f\"--no-version-check \" \\\\\\n        \\n        return bash_command\\n\\n    @task.bash\\n    def build_marts_models() -> str:\\n        bash_command=f\"dbt run --profiles-dir {PROJECT_DIR} \" \\\\\\n                             f\"--project-dir {PROJECT_DIR} \" \\\\\\n                             f\"--select tag:marts\" \\\\\\n                             f\"--no-version-check \" \\\\\\n        \\n        return bash_command\\n  \\n    # последовательность задач\\n    moving_data_from_source_to_dwh = moving_data_from_source_to_dwh()\\n    build_staging_models = build_staging_models()\\n    build_intermediate_models = build_intermediate_models()\\n    build_marts_models = build_marts_models()\\n    \\n    (\\n        moving_data_from_source_to_dwh\\n        >> build_staging_models\\n        >> build_intermediate_models\\n        >> build_marts_models\\n    )\\n\\n\\nairflow_pipeline = airflow_pipeline()'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "a0a678b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from datetime import datetime, timedelta\\n\\nfrom airflow.decorators import dag, task\\n\\n\\nPROJECT_DIR = \"/opt/airflow/dbt\"\\nDATA_PATH = f\"{PROJECT_DIR}/sample\"\\nSEED_PATH = f\"{PROJECT_DIR}/seeds\"\\n\\nDEFAULT_ARGS = {\\n    \"owner\": \"airflow\",\\n    \"depends_on_past\": False,\\n    \"email_on_failure\": False,\\n    \"retries\": 1,\\n    \"retry_delay\": timedelta(minutes=5)\\n}\\n\\n\\n@dag(\\n    default_args=DEFAULT_ARGS,\\n    max_active_runs=1,\\n    schedule_interval=\"0 1 * * *\"0 * * * *\",\\n    start_date=datetime(2025, 2, 1),\\n    catchup=True\\n)\\ndef airflow_pipeline():\\n\\n    @task\\n    def moving_data_from_source_to_dwh(**context) -> None:\\n        \"\"\"\\n        Извлекает данные из таблицы \\'orders\\' в источнике PostgreSQL\\n        и загружает их в таблицу \\'orders\\' в ClickHouse DWH.\\n\\n        Предполагается, что структура таблицы \\'orders\\' в ClickHouse\\n        соответствует схеме источника:\\n        (order_id Int64, product_id Int64, timestamp DateTime, customer_id Int64, money Decimal).\\n        \"\"\"\\n        import pandas as pd\\n        from airflow.providers.postgres.hooks.postgres import PostgresHook\\n        from airflow.providers.clickhouse.hooks.clickhouse import ClickHouseHook\\n\\n        # Define connection IDs and table names\\n        postgres_conn_id = \"postgres_source\"  # Replace with your actual Postgres connection ID\\n        clickhouse_conn_id = \"clickhouse_dwh\"\\n        source_table = \"orders\"\\n        target_table = \"orders\" # Assuming the target table has the same name\\n\\n        # Define source schema explicitly for clarity, though hook handles types\\n        source_schema = {\\n            \\'order_id\\': \\'Int64\\',\\n            \\'product_id\\': \\'Int64\\',\\n            \\'timestamp\\': \\'datetime64[ns]\\', # Pandas equivalent for datetime\\n            \\'customer_id\\': \\'Int64\\',\\n            \\'money\\': \\'object\\' # Pandas often reads numeric/decimal as object initially\\n        }\\n        target_columns = [\\'order_id\\', \\'product_id\\', \\'timestamp\\', \\'customer_id\\', \\'money\\']\\n\\n        # Create hooks\\n        pg_hook = PostgresHook(postgres_conn_id=postgres_conn_id)\\n        ch_hook = ClickHouseHook(clickhouse_conn_id=clickhouse_conn_id)\\n\\n        # Extract data from PostgreSQL\\n        sql = f\"SELECT {\\', \\'.join(target_columns)} FROM {source_table}\"\\n        source_conn = pg_hook.get_conn()\\n\\n        try:\\n            # Use pandas read_sql for better type handling potential with numeric/decimal\\n            df = pd.read_sql(sql, source_conn)\\n\\n            # Optional: Explicit type casting if needed, especially for decimals\\n            # df[\\'money\\'] = pd.to_numeric(df[\\'money\\']) # Cast if read as string/object\\n            # Ensure correct types based on source schema if read_sql inference isn\\'t perfect\\n            # df = df.astype(source_schema, errors=\\'ignore\\') # Apply schema types\\n\\n        finally:\\n            if source_conn:\\n                source_conn.close()\\n\\n        # Load data into ClickHouse\\n        if not df.empty:\\n            # Convert DataFrame to list of lists/tuples for insertion\\n            # Handle potential NaT values for datetime columns if necessary before conversion\\n            df[\\'timestamp\\'] = df[\\'timestamp\\'].astype(object).where(df[\\'timestamp\\'].notnull(), None)\\n            rows_to_insert = df.values.tolist()\\n\\n            ch_hook.insert_rows(\\n                table=target_table,\\n                rows=rows_to_insert,\\n                target_fields=target_columns\\n            )\\n        else:\\n            # Optionally log that no data was found\\n            pass # Or use context[\\'ti\\'].log.info(\"No data extracted from source.\")\\n\\n    @task.bash\\n    def build_staging_models() -> str:\\n        bash_command=f\"dbt run --profiles-dir {PROJECT_DIR} \" \\\\\\n                             f\"--project-dir {PROJECT_DIR} \" \\\\\\n                             f\"--select tag:stage\" \\\\\\n                             f\"--no-version-check \" \\\\\\n        \\n        return bash_command\\n    \\n    @task.bash\\n    def build_intermediate_models() -> str:\\n        bash_command=f\"dbt run --profiles-dir {PROJECT_DIR} \" \\\\\\n                             f\"--project-dir {PROJECT_DIR} \" \\\\\\n                             f\"--select tag:intermediate\" \\\\\\n                             f\"--no-version-check \" \\\\\\n        \\n        return bash_command\\n\\n    @task.bash\\n    def build_marts_models() -> str:\\n        bash_command=f\"dbt run --profiles-dir {PROJECT_DIR} \" \\\\\\n                             f\"--project-dir {PROJECT_DIR} \" \\\\\\n                             f\"--select tag:marts\" \\\\\\n                             f\"--no-version-check \" \\\\\\n        \\n        return bash_command\\n  \\n    # последовательность задач\\n    moving_data_from_source_to_dwh = moving_data_from_source_to_dwh()\\n    build_staging_models = build_staging_models()\\n    build_intermediate_models = build_intermediate_models()\\n    build_marts_models = build_marts_models()\\n    \\n    (\\n        moving_data_from_source_to_dwh\\n        >> build_staging_models\\n        >> build_intermediate_models\\n        >> build_marts_models\\n    )\\n\\n\\nairflow_pipeline = airflow_pipeline()'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def indent_code_block(code: str, indent: int) -> str:\n",
    "    lines = code.splitlines()\n",
    "    if not lines:\n",
    "        return \"\"\n",
    "    first_line = lines[0]\n",
    "    indented_lines = [(\" \" * indent) + line if line.strip() else \"\" for line in lines[1:]]\n",
    "    return \"\\n\".join([first_line] + indented_lines)\n",
    "\n",
    "cleaned_code_moving = indent_code_block(cleaned_code_moving, indent=4)\n",
    "# ПОДСТАВЛЯЕМ функцию moving_data_from_source_to_dwh\n",
    "template = re.sub(\n",
    "    r\"def moving_data_from_source_to_dwh\\(\\*\\*context\\) -> None:\\n\\s*pass\",\n",
    "    cleaned_code_moving,\n",
    "    template\n",
    ")\n",
    "\n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "499b3a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAG saved to results/airflow_dag/airflow_filled.py\n"
     ]
    }
   ],
   "source": [
    "# Сохраняем итоговый файл\n",
    "dag_path = \"results/airflow_dag/airflow_filled.py\"\n",
    "with open(dag_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(template)\n",
    "print(f\"DAG saved to {dag_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109ad104",
   "metadata": {},
   "source": [
    "# Генерация кода для DBT проекта"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898592d5",
   "metadata": {},
   "source": [
    "#### sources.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b863506d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class DataSource(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    data_schema: Dict[str, str] # column_name: type\n",
    "    type: str  # 'table', 'csv', 'api', etc.\n",
    "    database: str\n",
    "    access_method: str | None = None\n",
    "    limitations: Optional[str] | None = None\n",
    "    recommendations: List[str] | None = None\n",
    "    connection_params: Dict[str, str] | None = None\n",
    "\n",
    "\n",
    "ds_orders = DataSource(\n",
    "    name=\"orders\",\n",
    "    description=\"Таблица заказов\",\n",
    "    data_schema={\"order_id\": \"Int64\",\n",
    "                 \"product_id\": \"Int64\",\n",
    "                 \"timestamp\": \"datetime\",\n",
    "                 \"customer_id\": \"Int64\",\n",
    "                 \"money\": \"numeric\"},\n",
    "    type=\"table\",\n",
    "    database=\"PostgreSQL\",\n",
    "    access_method=None,\n",
    "    recommendations=[\"использовать фильтрацию по дате (timestamp)\"]\n",
    ")\n",
    "ds_customers = DataSource(\n",
    "    name=\"customers\",\n",
    "    description=\"Таблица клиентов\",\n",
    "    data_schema={\"region_id\": \"Int64\",\n",
    "                 \"registration_date\": \"datetime\",\n",
    "                 \"customer_id\": \"Int64\"},\n",
    "    type=\"table\",\n",
    "    database=\"PostgreSQL\",\n",
    "    access_method=None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fe92945",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_yaml = \"\"\n",
    "for col_name, col_type in ds_orders.data_schema.items():\n",
    "    columns_yaml += f\"        - name: {col_name}\\n\" \\\n",
    "                    f\"          data_type: {col_type}\\n\"\n",
    "\n",
    "sources_yml = f\"\"\"version: 2\n",
    "\n",
    "sources:\n",
    "- name: exported_data\n",
    "  tables:\n",
    "    - name: {ds_orders.name}\n",
    "      description: \"{ds_orders.description}\"\n",
    "      columns:\\n\"\"\" + f\"{columns_yaml}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6c4ebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/dbt/models/sources.yml\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(sources_yml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c809641f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 вариант\n",
    "source_dict = {\n",
    "    'version': 2,\n",
    "    'sources': [\n",
    "        {\n",
    "            'name': 'exported_data',\n",
    "            'tables': [\n",
    "                {'name': ds_orders.name,\n",
    "                 'description': ds_orders.description,\n",
    "                 'columns': ds_orders.data_schema},\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30211ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"results/dbt/models/sources2.yml\", 'w', encoding='utf-8') as f:\n",
    "    yaml.dump(source_dict, f, sort_keys=False, allow_unicode=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3256a5e",
   "metadata": {},
   "source": [
    "#### profiles.yml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50858b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 вариант\n",
    "profiles_dict = {\n",
    "    'airflow': {\n",
    "      \"target\": \"dev\",\n",
    "      \"outputs\": {\n",
    "        \"dev\": {\n",
    "          \"type\": \"postgresql\",\n",
    "          \"host\": \"localhost\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "}\n",
    "\n",
    "import yaml\n",
    "\n",
    "with open(\"results/dbt/models/profiles.yml\", 'w', encoding='utf-8') as f:\n",
    "    yaml.dump(profiles_dict, f, sort_keys=False, allow_unicode=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b63d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f520322",
   "metadata": {},
   "source": [
    "#### stage models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a224fff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds_orders' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 18\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(match\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m matches)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m code_str\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     12\u001b[0m sources \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msources\u001b[39m\u001b[38;5;124m'\u001b[39m: [\n\u001b[1;32m     15\u001b[0m         {\n\u001b[1;32m     16\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexported_data\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     17\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtables\u001b[39m\u001b[38;5;124m'\u001b[39m: [\n\u001b[0;32m---> 18\u001b[0m                 {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mds_orders\u001b[49m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_last_data\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     19\u001b[0m                  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124midentifier\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morders_last_data\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m                  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m: ds_orders\u001b[38;5;241m.\u001b[39mdescription},\n\u001b[1;32m     21\u001b[0m             ]\n\u001b[1;32m     22\u001b[0m         }\n\u001b[1;32m     23\u001b[0m     ]\n\u001b[1;32m     24\u001b[0m }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ds_orders' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_sql_code(code_str: str) -> str:\n",
    "    # убрать обрамление ``` или ```python и оставить только содержимое\n",
    "    pattern = r\"```(?:sql)?\\n(.*?)```\"\n",
    "    matches = re.findall(pattern, code_str, re.DOTALL)\n",
    "    if matches:\n",
    "        # если несколько блоков, объединяем их через 2 перевода строки\n",
    "        return \"\\n\\n\".join(match.strip() for match in matches)\n",
    "    return code_str.strip()\n",
    "\n",
    "sources = {\n",
    "    'version': 2,\n",
    "    'sources': [\n",
    "        {\n",
    "            'name': 'exported_data',\n",
    "            'tables': [\n",
    "                {'name': ds_orders.name + \"_last_data\",\n",
    "                 'identifier': \"orders_last_data\",\n",
    "                 'description': ds_orders.description},\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46460ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'version': 2,\n",
       " 'sources': [{'name': 'exported_data',\n",
       "   'tables': [{'name': 'orders_last_data',\n",
       "     'identifier': 'orders_last_data',\n",
       "     'description': 'Таблица заказов'}]}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54ee3530",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import yaml\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "MODEL = \"just-ai/openai-proxy/gpt-4o\"\n",
    "TEMPERATURE = 0\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=MODEL,\n",
    "    temperature=TEMPERATURE,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=TOKEN,\n",
    "    base_url=\"https://caila.io/api/adapters/openai\"\n",
    ")\n",
    "\n",
    "system_template = (\n",
    "    \"You are a data engineer. \"\n",
    "    \"Generate a dbt model for the 'stage' layer in the medallion architecture.\"\n",
    "    \"The model should select all columns from the raw table in the analytical DWH using dbt's source() function.\"\n",
    "    \"Add a config block at the top with materialized='view'\"\n",
    ")\n",
    "\n",
    "user_template = (\n",
    "    \"Here is the dbt sources.yml content: {sources}\\n\"\n",
    "    \"- Do not add any comments, code or explanations outside the function code.\\n\"\n",
    "    \"- Return only the SQL code.\"\n",
    ")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template),\n",
    "     (\"user\", user_template)]\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm\n",
    "\n",
    "result = chain.invoke(\n",
    "    {\"sources\": yaml.dump(sources, allow_unicode=True)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c2edff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_sql_code = clean_sql_code(result.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5bc24fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_path = \"results/dbt/models/stage.sql\"\n",
    "with open(stage_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(cleaned_sql_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81c51fe",
   "metadata": {},
   "source": [
    "#### stage v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dict = {\n",
    "    'version': 2,\n",
    "    'sources': [\n",
    "        {\n",
    "            'name': 'exported_data',\n",
    "            'schema': 'last',\n",
    "            'tables': [\n",
    "                {'name': ds_orders.name,\n",
    "                 'identifier': ds_orders.name + '_last_data',\n",
    "                 'description': ds_orders.description,\n",
    "                 'columns': ds_orders.data_schema},\n",
    "                {'name': ds_customers.name,\n",
    "                 'identifier': ds_customers.name + '_last_data',\n",
    "                 'description': ds_customers.description,\n",
    "                 'columns': ds_customers.data_schema}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1678538b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'orders',\n",
       " 'identifier': 'orders_last_data',\n",
       " 'description': 'Таблица заказов',\n",
       " 'columns': {'order_id': 'Int64',\n",
       "  'product_id': 'Int64',\n",
       "  'timestamp': 'datetime',\n",
       "  'customer_id': 'Int64',\n",
       "  'money': 'numeric'}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_dict['sources'][0]['tables'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "83b7f29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import yaml\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "MODEL = \"just-ai/openai-proxy/gpt-4o\"\n",
    "TEMPERATURE = 0\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=MODEL,\n",
    "    temperature=TEMPERATURE,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=TOKEN,\n",
    "    base_url=\"https://caila.io/api/adapters/openai\"\n",
    ")\n",
    "\n",
    "\n",
    "class ModelMetadata(BaseModel):\n",
    "    sql_code: str\n",
    "    schema_yml: dict\n",
    "\n",
    "# parser = JsonOutputParser(pydantic_object=ModelMetadata)\n",
    "parser = JsonOutputParser()\n",
    "    \n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"  \n",
    "    Ты опытный дата-инженер, реализуешь dbt-модели для stage в трёхслойной архитектуре аналитического хранилища, построенного в Clickhouse.\n",
    "    Сгенерируй dbt-модели, включая:\n",
    "    1. SQL-код (вместе с config)\n",
    "    2. Описание для внесения в schema.yml (включая тесты, если они необходимы)\n",
    "    3. Добавь метку о времени и/или другие метаданные, полезные на слое stage\n",
    "     \n",
    "    Структура вывода (строго в JSON):\n",
    "        {{\n",
    "            \"stg_model_name1\": {{\n",
    "                \"sql_code\": \"SQL-код модели\",\n",
    "                \"schema_yml\": \"описание модели в формате JSON для schema.yml\"\n",
    "            }},\n",
    "            \"stg_model_name2\": {{\n",
    "                \"sql_code\": \"SQL-код модели\",\n",
    "                \"schema_yml\": \"описание модели в формате JSON для schema.yml\"\n",
    "            }},\n",
    "            ...\n",
    "        }}\n",
    "    \"\"\"),\n",
    "    (\"user\", \"\"\"\n",
    "    Создай dbt-модели для stage-слоя на основе sources.yml: {source}.\n",
    "    Формат вывода: JSON\n",
    "    \"\"\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "    \n",
    "result = chain.invoke({\n",
    "        \"source\": source_dict\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stg_orders': {'sql_code': \"{% set source_table = source('exported_data', 'orders') %}\\n\\nselect\\n    order_id,\\n    product_id,\\n    timestamp,\\n    customer_id,\\n    money,\\n    now() as loaded_at\\nfrom {{ source_table }}\",\n",
       "  'schema_yml': {'version': 2,\n",
       "   'models': [{'name': 'stg_orders',\n",
       "     'description': 'Stage layer for orders data',\n",
       "     'columns': [{'name': 'order_id',\n",
       "       'description': 'Unique identifier for the order',\n",
       "       'tests': ['unique', 'not_null']},\n",
       "      {'name': 'product_id',\n",
       "       'description': 'Identifier of the product ordered',\n",
       "       'tests': ['not_null']},\n",
       "      {'name': 'timestamp',\n",
       "       'description': 'Timestamp of the order',\n",
       "       'tests': ['not_null']},\n",
       "      {'name': 'customer_id',\n",
       "       'description': 'Identifier of the customer who made the order',\n",
       "       'tests': ['not_null']},\n",
       "      {'name': 'money',\n",
       "       'description': 'Amount of money involved in the order',\n",
       "       'tests': ['not_null']},\n",
       "      {'name': 'loaded_at',\n",
       "       'description': 'Timestamp when the record was loaded into the stage table',\n",
       "       'tests': []}]}]}},\n",
       " 'stg_customers': {'sql_code': \"{% set source_table = source('exported_data', 'customers') %}\\n\\nselect\\n    region_id,\\n    registration_date,\\n    customer_id,\\n    now() as loaded_at\\nfrom {{ source_table }}\",\n",
       "  'schema_yml': {'version': 2,\n",
       "   'models': [{'name': 'stg_customers',\n",
       "     'description': 'Stage layer for customers data',\n",
       "     'columns': [{'name': 'region_id',\n",
       "       'description': 'Region identifier for the customer',\n",
       "       'tests': ['not_null']},\n",
       "      {'name': 'registration_date',\n",
       "       'description': 'Date when the customer registered',\n",
       "       'tests': ['not_null']},\n",
       "      {'name': 'customer_id',\n",
       "       'description': 'Unique identifier for the customer',\n",
       "       'tests': ['unique', 'not_null']},\n",
       "      {'name': 'loaded_at',\n",
       "       'description': 'Timestamp when the record was loaded into the stage table',\n",
       "       'tests': []}]}]}}}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "899a1089",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_path = \"results/dbt/models/stage_orders.sql\"\n",
    "with open(stage_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(result['stg_orders']['sql_code'])\n",
    "\n",
    "stage_path = \"results/dbt/models/stage_customers.sql\"\n",
    "with open(stage_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(result['stg_customers']['sql_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "26bd9d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_schema_path = \"results/dbt/models/schema_stage.yml\"\n",
    "with open(stage_schema_path, 'w', encoding='utf-8') as f:\n",
    "    yaml.dump(result[\"stg_orders\"][\"schema_yml\"], f, sort_keys=False, allow_unicode=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
